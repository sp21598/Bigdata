Scenario Based questions:

Que-> Will the reducer work or not if you use “Limit 1” in any HiveQL query?

Ans-> Yes reducers will be called even if you use Limit 1 if the query as aggregations or joins.
If it's a simple select query or small query which doesn't use any complex operations then no reducer will work.


Que-> Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. 
Then, what will happen if we have multiple clients trying to access Hive at the same time? 

Ans->The default metastore configuration only allows to set one connection at a tim.If you want to set 
another connecion then you have to disconnect the first one.For multiple clients we have to use external
connection which will serve as metstore and allow multiple clients.

Que-> Suppose, I create a table that contains details of all the transactions done by the customers: 
CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. 
But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?

Ans-> Partitioning can be applied based on each month and hence only for a particular partition the data will get scanned and processing time will be reduced
as the above table is not partitioned we have to create a new partitioned table and set parameters such as hive.exec.dynamic.partition=true 
and set hive.exec.dynamic.partition.mode=nonstrict and then transfer data from transaction_details(table without partitions) to this newly generated partitioned table 
and then we can query it;

Que-> How can you add a new partition for the month December in the above partitioned table?

Ans-> For adding a new partition in the above table transaction_details, the syntax is as such:
ALTER TABLE (newly created partioned table name) ADD PARTITION (condition given,i.e for this case we can say month="DECEMBER") LOCATION  ‘/hdfs_path_of_directory’;


Que-> I am inserting data into a table based on partitions dynamically. 
But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?

Ans-> I think we can't directly insert data ino partitioned table we have to insert data into partitioned table via a normal table and also we have to set the following:-
set hive.exec.dynamic.partition.mode=nonstrict and set hive.exec.dynamic.partition=true.In this mode hive knows that it doesn't need any static partition column.


Que-> Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?

Ans-> We can solve this by using ‘org.apache.hadoop.hive.serde2.OpenCSVSerde’ library which we can add by downloading it's corresponding jar file 
and adding it using ROW FORMAT SERDE property while creating a table for it.

Que-> Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. 
The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?

Ans-> By storing small files which are less than block size in haddo the number of metadata entries in namenode increases and also reading each file through a whole round-trip increase
the overall overhead.So for this a sequence file is used which acts as a container for small files.Sequence files are made of binary key-value pairs. 
When Hive converts queries to MapReduce jobs, it decides on the appropriate key-value pairs to be used for a given record. 
So when we use sequence files it clubs or merges two small files into one file kind f operation.
In Hive we can create a sequence file by specifying STORED AS SEQUENCEFILE in the end of a CREATE TABLE statement.

Que-> LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;
The following statement failed to execute. What can be the cause?

Ans->LOAD DATA LOCAL INPATH ‘Home/country/state/(filename is required)’
OVERWRITE INTO TABLE address;
Let's assume our file is sales.csv
then LOAD DATA LOCAL INPATH ‘Home/country/state/sales.csv’
OVERWRITE INTO TABLE address;

Que-> Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?

Ans-> Yes, we can add the nodes by following the below steps:

Step 1: Take a new system; create a new username and password
Step 2: Install SSH and with the master node setup SSH connections
Step 3: Add ssh public_rsa id key to the authorized keys file
Step 4: Add the new DataNode hostname, IP address, and other details in /etc/hosts slaves file:
Step 5: Start the DataNode on a new node
Step 6: Login to the new node like suhadoop or:
Step 7: Start HDFS of the newly added slave node by using the following command:
Step 8: Check the output of the jps command on the new node




